!pip install transformers datasets torchaudio
!pip install transformers torchaudio ffmpeg-python librosa

import torch
import soundfile as sf
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import numpy as np
import torchaudio
import librosa

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

model_name = "openai/whisper-large-v2"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)

def transcribe_audio(audio_path, ):
    audio_input, sample_rate = sf.read(audio_path)
    audio_input = audio_input.astype(np.float32)
    if sample_rate != 16000:
        import torchaudio
        audio_tensor = torch.from_numpy(audio_input)
        resampler = torchaudio.transforms.Resample(sample_rate, 16000)
        resampled_audio = resampler(audio_tensor).numpy()
    else:
        resampled_audio = audio_input
    input_features = processor(resampled_audio, sampling_rate=16000, return_tensors="pt").input_features.to(device)
    forced_decoder_ids = processor.get_decoder_prompt_ids(language='en', task="transcribe")
    with torch.no_grad():
        predicted_ids = model.generate(
            input_features,
            forced_decoder_ids=forced_decoder_ids,
            max_length=448,
        )
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

def transcribe_audio_chunks(audio_path, chunk_duration=5):
    try:
        audio, sr = librosa.load(audio_path, sr=16000) #Removed duration limit
    except Exception as e:
        print(f"Error loading audio: {e}")
        return None

    num_chunks = int(len(audio) / (sr * chunk_duration))
    transcriptions = []
    for i in range(num_chunks):
        start = i * sr * chunk_duration
        end = min((i + 1) * sr * chunk_duration, len(audio))
        chunk = audio[start:end]
        input_features = processor(chunk, sampling_rate=16000, return_tensors="pt").input_features.to(device)
        forced_decoder_ids = processor.get_decoder_prompt_ids(language='mr', task="transcribe") #Specify language
        with torch.no_grad():
            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids, max_length=448)
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        transcriptions.append(transcription)

    remaining_audio = audio[num_chunks*sr*chunk_duration:]

    if len(remaining_audio) > 0:
        input_features = processor(remaining_audio, sampling_rate=16000, return_tensors="pt").input_features.to(device)
        forced_decoder_ids = processor.get_decoder_prompt_ids(language='mr', task="transcribe")

        with torch.no_grad():
            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids, max_length=448)

        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        transcriptions.append(transcription)

    return transcriptions

audio_file_path = "/content/drive/MyDrive/call_recording.wav"
transcriptions = transcribe_audio_chunks(audio_file_path)

if transcriptions:
    for i, transcription in enumerate(transcriptions):
        print(f"Chunk {i+1}: {transcription}")
